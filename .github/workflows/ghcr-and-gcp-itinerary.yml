name: Docker push to GHCR, GCP Artifact Registry and deploy Backend Infrastructure

on:
  push:
    branches: [ "main", "feature/exercise8-leo" ]
    tags: [ 'v*.*.*' ]
    paths:
      - 'kubernetes/**'
      - 'Dockerfile'
      - '.github/workflows/ghcr-and-gcp-itinerary.yml'
      - 'src/**'
  workflow_dispatch:

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    if: ${{ github.event.head_commit.message != 'skip-build' }}
    permissions:
      contents: read
      packages: write
      id-token: write  # needed for GCP Workload Identity

    outputs:
      backend_image: ${{ steps.export_image.outputs.backend_image }}
      app_version: ${{ steps.export_app_version.outputs.app_version }}

    steps:
      - uses: actions/checkout@v4

      # Login to GitHub Container Registry
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # Login to Google Artifact Registry
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Configure Docker for GCP
        run: gcloud auth configure-docker europe-west1-docker.pkg.dev

      # Build and push to both registries
      - name: Build and push to GHCR and GCP
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: |
            ghcr.io/${{ github.repository }}:latest
            ghcr.io/${{ github.repository }}:${{ github.sha }}
            europe-west1-docker.pkg.dev/graphite-plane-474510-s9/docker-repo/travel-backend:latest
            europe-west1-docker.pkg.dev/graphite-plane-474510-s9/docker-repo/travel-backend:${{ github.sha }}

      - name: Export image reference
        id: export_image
        run: echo "backend_image=europe-west1-docker.pkg.dev/graphite-plane-474510-s9/docker-repo/travel-backend:${GITHUB_SHA}" >> "$GITHUB_OUTPUT"

      - name: Export app version
        id: export_app_version
        run: echo "app_version=${GITHUB_SHA}" >> "$GITHUB_OUTPUT"
  deploy:
    # needs: build-and-push
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    steps:
      - uses: actions/checkout@v4

      # Authenticate again for kubectl
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Install GKE Auth Plugin
        run: |
          gcloud components install gke-gcloud-auth-plugin --quiet

      - name: Configure kubectl for GKE
        run: |
          gcloud container clusters get-credentials tripico-cluster --region europe-west1

      - name: Install Helm
        uses: azure/setup-helm@v4
        with:
          version: 'latest'

      - name: Install NGINX Ingress Controller (if not exists)
        run: |
          # Check if ingress-nginx namespace exists
          if ! kubectl get namespace ingress-nginx &>/dev/null; then
            echo "Installing NGINX Ingress Controller..."
            helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
            helm repo update
            helm install ingress-nginx ingress-nginx/ingress-nginx \
              --namespace ingress-nginx \
              --create-namespace \
              --wait --timeout 5m
          else
            echo "NGINX Ingress Controller already installed"
          fi

      # # Cleanup Step - Uncomment if needed (if anything in the cluster is not managed by Helm and must be replaced)
      # - name: Clean up pre-existing resources (if not managed by Helm)
      #   run: |
      #     echo "Checking for non-Helm managed resources..."
      #           # Function to check and delete resource if not managed by Helm
      #     cleanup_resource() {
      #       local resource_type=$1
      #       local resource_name=$2
      #       if kubectl get $resource_type $resource_name &>/dev/null; then
      #         local managed_by=$(kubectl get $resource_type $resource_name -o jsonpath='{.metadata.labels.app\.kubernetes\.io/managed-by}' 2>/dev/null || echo "")
      #         if [ "$managed_by" != "Helm" ]; then
      #           echo "Found $resource_type/$resource_name not managed by Helm, deleting..."
      #           # Use --wait=false for PVC to avoid hanging, timeout for others
      #           if [ "$resource_type" == "pvc" ]; then
      #             kubectl delete $resource_type $resource_name --wait=false --timeout=30s 2>/dev/null || true
      #           else
      #             kubectl delete $resource_type $resource_name --timeout=30s 2>/dev/null || true
      #           fi
      #           echo "Deleted $resource_type/$resource_name"
      #         else
      #           echo "$resource_type/$resource_name is already managed by Helm"
      #         fi
      #       else
      #         echo "No pre-existing $resource_type/$resource_name found"
      #       fi
      #     }
      #     # Clean up all resources that might conflict
      #     # IMPORTANT: Delete in correct order to avoid hanging
      #     # 1. First delete ingress (no dependencies)
      #     cleanup_resource "ingress" "travel-warnings-ingress"
      #     # 2. Delete deployments (this releases PVC mounts)
      #     cleanup_resource "deployment" "travel-warnings-api"
      #     cleanup_resource "deployment" "warnings-postgres"
      #     # 3. Wait for pods to terminate (important!)
      #     echo "Waiting for pods to terminate..."
      #     kubectl wait --for=delete pod -l app=travel-warnings-api --timeout=60s 2>/dev/null || true
      #     kubectl wait --for=delete pod -l app=warnings-postgres --timeout=60s 2>/dev/null || true
      #     # 4. Now safe to delete PVC (no longer mounted)
      #     cleanup_resource "pvc" "warnings-postgres-data-pvc"
      #     # 5. Delete services (no dependencies)
      #     cleanup_resource "service" "travel-warnings-api"
      #     cleanup_resource "service" "warnings-postgres"
      #     # 6. Delete configmaps and secrets (no dependencies)
      #     cleanup_resource "configmap" "travel-warnings-config"
      #     cleanup_resource "secret" "smtp-secret"
      #     # 7. Delete certificate resource (the Certificate CR, not cert-manager itself!)
      #     cleanup_resource "certificate" "warnings-tls-cert"
      #     echo "Cleanup complete!"

      - name: Reset cert-manager namespace when unmanaged leftovers exist
        run: |
          if kubectl get namespace cert-manager &>/dev/null; then
            if helm list --namespace cert-manager | grep -q "^cert-manager"; then
              echo "cert-manager release already managed by Helm; skipping namespace reset"
            else
              echo "Namespace cert-manager exists without a Helm release; removing namespace to clean leftovers..."
              kubectl delete namespace cert-manager --wait=false || true
              echo "Waiting up to 3 minutes for namespace deletion"
              kubectl wait --for=delete namespace/cert-manager --timeout=180s || true
            fi
          else
            echo "cert-manager namespace absent; nothing to reset"
          fi

      - name: Clean legacy cert-manager resources
        run: |
          if kubectl get namespace cert-manager &>/dev/null; then
            echo "Removing leftover cert-manager service accounts and configmaps so Helm can own them..."
            kubectl delete serviceaccount cert-manager -n cert-manager --ignore-not-found
            kubectl delete serviceaccount cert-manager-cainjector -n cert-manager --ignore-not-found
            kubectl delete serviceaccount cert-manager-webhook -n cert-manager --ignore-not-found
            kubectl delete configmap cert-manager -n cert-manager --ignore-not-found
          else
            echo "cert-manager namespace does not exist yet; nothing to clean"
          fi

      - name: Remove legacy cert-manager CRDs when release is absent
        run: |
          if helm list --namespace cert-manager | grep -q "^cert-manager"; then
            echo "Helm already manages cert-manager; skipping CRD cleanup"
          else
            echo "Checking for unmanaged cert-manager CRDs to delete..."
            CRDS=(
              certificaterequests.cert-manager.io
              certificates.cert-manager.io
              challenges.acme.cert-manager.io
              clusterissuers.cert-manager.io
              issuers.cert-manager.io
              orders.acme.cert-manager.io
            )
            for crd in "${CRDS[@]}"; do
              if kubectl get crd "$crd" &>/dev/null; then
                echo "Deleting CRD $crd"
                kubectl delete crd "$crd" --wait=false || true
              else
                echo "CRD $crd already absent"
              fi
            done
            echo "Giving Kubernetes time to finish CRD deletions"
            sleep 10
          fi

      - name: Remove legacy cert-manager cluster-scoped resources
        run: |
          if helm list --namespace cert-manager | grep -q "^cert-manager"; then
            echo "Helm already manages cert-manager; skipping cluster resource cleanup"
          else
            echo "Deleting ALL unmanaged cert-manager cluster-scoped resources..."

            # Delete all ClusterRoles that start with cert-manager
            CLUSTERROLES=$(kubectl get clusterrole -o name | grep cert-manager || true)
            if [ -n "$CLUSTERROLES" ]; then
              echo "Deleting ClusterRoles: $CLUSTERROLES"
              echo "$CLUSTERROLES" | xargs kubectl delete --ignore-not-found || true
            else
              echo "No cert-manager ClusterRoles found"
            fi

            # Delete all ClusterRoleBindings that start with cert-manager
            CLUSTERROLEBINDINGS=$(kubectl get clusterrolebinding -o name | grep cert-manager || true)
            if [ -n "$CLUSTERROLEBINDINGS" ]; then
              echo "Deleting ClusterRoleBindings: $CLUSTERROLEBINDINGS"
              echo "$CLUSTERROLEBINDINGS" | xargs kubectl delete --ignore-not-found || true
            else
              echo "No cert-manager ClusterRoleBindings found"
            fi

            # Delete ValidatingWebhookConfigurations
            VALIDATING_WEBHOOKS=$(kubectl get validatingwebhookconfiguration -o name | grep cert-manager || true)
            if [ -n "$VALIDATING_WEBHOOKS" ]; then
              echo "Deleting ValidatingWebhookConfigurations: $VALIDATING_WEBHOOKS"
              echo "$VALIDATING_WEBHOOKS" | xargs kubectl delete --ignore-not-found || true
            else
              echo "No cert-manager ValidatingWebhookConfigurations found"
            fi

            # Delete MutatingWebhookConfigurations
            MUTATING_WEBHOOKS=$(kubectl get mutatingwebhookconfiguration -o name | grep cert-manager || true)
            if [ -n "$MUTATING_WEBHOOKS" ]; then
              echo "Deleting MutatingWebhookConfigurations: $MUTATING_WEBHOOKS"
              echo "$MUTATING_WEBHOOKS" | xargs kubectl delete --ignore-not-found || true
            else
              echo "No cert-manager MutatingWebhookConfigurations found"
            fi

            echo "Waiting for cleanup to complete..."
            sleep 5
          fi

      - name: Remove legacy cert-manager Roles and RoleBindings across all namespaces
        run: |
          if helm list --namespace cert-manager | grep -q "^cert-manager"; then
            echo "Helm already manages cert-manager; skipping namespace cleanup"
          else
            echo "Deleting cert-manager Roles and RoleBindings across all namespaces..."

            # Get all namespaces and clean up cert-manager resources
            for ns in $(kubectl get namespaces -o jsonpath='{.items[*].metadata.name}'); do
              echo "Checking namespace: $ns"

              # Delete Roles
              ROLES=$(kubectl get role -n "$ns" -o name 2>/dev/null | grep cert-manager || true)
              if [ -n "$ROLES" ]; then
                echo "  Deleting Roles in $ns: $ROLES"
                echo "$ROLES" | xargs kubectl delete -n "$ns" --ignore-not-found || true
              fi

              # Delete RoleBindings
              ROLEBINDINGS=$(kubectl get rolebinding -n "$ns" -o name 2>/dev/null | grep cert-manager || true)
              if [ -n "$ROLEBINDINGS" ]; then
                echo "  Deleting RoleBindings in $ns: $ROLEBINDINGS"
                echo "$ROLEBINDINGS" | xargs kubectl delete -n "$ns" --ignore-not-found || true
              fi
            done

            echo "Namespace cleanup complete"
          fi

      - name: Create or Update cert-manager (without ClusterIssuer)
        working-directory: ./kubernetes
        run: |
          helm repo add jetstack https://charts.jetstack.io
          helm repo update
          helm dependency update ./cert-manager-chart
          helm dependency build ./cert-manager-chart
          if helm list --namespace cert-manager | grep -q "^cert-manager"; then
            echo "Updating cert-manager release..."
          else
            echo "Installing cert-manager release..."
          fi
          # Install cert-manager without ClusterIssuer first
          helm upgrade --install cert-manager ./cert-manager-chart \
            --namespace cert-manager \
            --create-namespace \
            --set clusterIssuer.enabled=false \
            --wait --timeout 5m

      - name: Wait for cert-manager webhook to be ready
        run: |
          echo "Waiting for cert-manager webhook deployment to be available..."
          kubectl wait --for=condition=Available --timeout=300s \
            deployment/cert-manager-webhook -n cert-manager

          echo "Waiting for cert-manager webhook to fully initialize..."
          # cert-manager webhook needs time to generate and trust its own certificate
          # We'll test if the webhook is ready by attempting a dry-run

          for i in {1..30}; do
            echo "Testing webhook readiness (attempt $i/30)..."
            if kubectl apply --dry-run=server -f - <<EOF 2>&1 | grep -q "created\|configured\|unchanged"; then
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: test-issuer
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: test@example.com
    privateKeySecretRef:
      name: test-issuer-key
    solvers:
    - http01:
        ingress:
          class: nginx
EOF
              echo "Webhook is ready and responding!"
              break
            fi
            echo "Webhook not ready yet, waiting 10 seconds..."
            sleep 10
          done

          echo "Verifying webhook pods are running..."
          kubectl get pods -n cert-manager -l app.kubernetes.io/name=webhook

      - name: Create ClusterIssuer
        working-directory: ./kubernetes
        run: |
          echo "Creating ClusterIssuer using kubectl apply..."
          # Template the ClusterIssuer from the helm chart
          helm template cert-manager ./cert-manager-chart \
            --namespace cert-manager \
            --set clusterIssuer.enabled=true \
            --show-only templates/clusterissuer-prod.yaml \
            | kubectl apply -f -

          echo "Verifying ClusterIssuer was created..."
          kubectl get clusterissuer

          echo "Waiting for ClusterIssuer to be ready..."
          sleep 10
          kubectl describe clusterissuer letsencrypt-prod

      - name: Verify Deployment
        run: |
          echo "=== Helm Release Status ==="
          helm status cert-manager --namespace cert-manager
          echo ""
          echo "=== Pod Status ==="
          kubectl get pods -l app.kubernetes.io/instance=cert-manager -o wide
          echo ""
          echo "=== Service Status ==="
          kubectl get svc -l app.kubernetes.io/instance=cert-manager
          echo ""
          echo "=== Ingress Status ==="
          kubectl get ingress
